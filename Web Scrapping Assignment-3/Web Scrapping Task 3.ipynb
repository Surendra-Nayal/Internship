{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4631f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "import time\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1210d",
   "metadata": {},
   "source": [
    "# Q1. Write a python program which searches all the product under a particular product from www.amazon.in. \n",
    "The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search \n",
    "for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take user input for search term\n",
    "search_term = input(\"Enter the product you want to search on Amazon.in: \")\n",
    "\n",
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "# opening the naukari page on selenium automated chrome browser\n",
    "driver.get(f\"https://www.amazon.in/s?k={search_term}\")\n",
    "\n",
    "# Create URL for search results page\n",
    "url = f\"https://www.amazon.in/s?k={search_term}\"\n",
    "\n",
    "# Send request to the URL and get the HTML response\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "\n",
    "# Parse the HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find all the products on the search results page\n",
    "products = soup.find_all(\"div\", {\"class\": \"s-result-item\"})\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998a7b79",
   "metadata": {},
   "source": [
    "# 2. In the above question, now scrape the following details of each product listed in first 3 pages of your \n",
    "search results and save it in a data frame and csv. In case if any product has less than 3 pages in search \n",
    "results then scrape all the products available under that product name. Details to be scraped are: \"Brand \n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and \n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a53582",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name=[]\n",
    "price=[]\n",
    "expected_delivery=[]\n",
    "product_url=[]\n",
    "\n",
    "start = 0\n",
    "end = 3\n",
    "for page in range(start,end):\n",
    "    pro_tag=driver.find_elements(By.XPATH,'//span[@class=\"a-size-base-plus a-color-base a-text-normal\"]')\n",
    "    for i in pro_tag:\n",
    "        pro_tag1=i.text\n",
    "        product_name.append(pro_tag1)\n",
    "    next_button = driver.find_elements(By.XPATH, 'a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]') #to scrap data from next pages\n",
    "    for button in next_button:\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "start = 0\n",
    "end = 3\n",
    "for page in range(start,end):\n",
    "    pri=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "    for i in pri:\n",
    "        pri1=i.text\n",
    "        price.append(pri1)\n",
    "    next_button = driver.find_elements(By.XPATH, 'a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]') #to scrap data from next pages\n",
    "    for button in next_button:\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "start = 0\n",
    "end = 3\n",
    "for page in range(start,end):\n",
    "    deli=driver.find_elements(By.XPATH,'//span[@class=\"a-color-base a-text-bold\"]')\n",
    "    for i in deli:\n",
    "        deli1=i.text\n",
    "        expected_delivery.append(deli1)\n",
    "    next_button = driver.find_elements(By.XPATH, 'a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]') #to scrap data from next pages\n",
    "    for button in next_button:\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "start = 0\n",
    "end = 3\n",
    "for page in range(start,end):\n",
    "    p_u=driver.find_elements(By.XPATH,'//a[@class = \"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]')\n",
    "    for i in p_u:\n",
    "        p_u1=i.get_attribute('href')\n",
    "        product_url.append(p_u1)\n",
    "    next_button = driver.find_elements(By.XPATH, 'a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]') #to scrap data from next pages\n",
    "    for button in next_button:\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "print(len(product_name),len(price),len(expected_delivery),len(product_url))\n",
    "174 177 171 177\n",
    "Brand=[]\n",
    "for p_u in product_url:\n",
    "    driver.get(p_u)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        brand=driver.find_element(By.XPATH,'//*[@id=\"productOverview_feature_div\"]/div/table/tbody/tr[1]/td[2]/span')\n",
    "        Brand.append(brand.text)\n",
    "    except NoSuchElementException:\n",
    "        Brand.append('-')\n",
    "df = pd.DataFrame({'Product Name':product_name, 'Price':price, 'Expected Delivery':expected_delivery, 'URL':product_url})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594b24e",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389aa64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "# opening the images.google.com page on selenium automated chrome browser\n",
    "driver.get(\"https://images.google.com/\")\n",
    "item=driver.find_element(By.CLASS_NAME,\"gLFyf\")\n",
    "item.send_keys('fruits')\n",
    "search=driver.find_element(By.XPATH,'//span[@class=\"z1asCe MZy1Rb\"]')\n",
    "search.click()\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    \n",
    "images=driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        break\n",
    "    print(\"Download {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])\n",
    "    file = open(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\IMG\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "# opening the naukari page on selenium automated chrome browser\n",
    "driver.get(\"https://images.google.com/\")\n",
    "\n",
    "item=driver.find_element(By.CLASS_NAME,\"gLFyf\")\n",
    "item.send_keys('cars')\n",
    "\n",
    "search=driver.find_element(By.XPATH,'//span[@class=\"z1asCe MZy1Rb\"]')\n",
    "search.click()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    \n",
    "images=driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        break\n",
    "    print(\"Download {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])\n",
    "    file = open(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\7. Webscrapping-3\\CARS\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f72393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "# opening the naukari page on selenium automated chrome browser\n",
    "driver.get(\"https://images.google.com/\")\n",
    "\n",
    "item=driver.find_element(By.CLASS_NAME,\"gLFyf\")\n",
    "item.send_keys('Machine Learning')\n",
    "\n",
    "search=driver.find_element(By.XPATH,'//span[@class=\"z1asCe MZy1Rb\"]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adb2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    \n",
    "images=driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        break\n",
    "    print(\"Download {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])\n",
    "    file = open(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\7. Webscrapping-3\\ML\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "# opening the naukari page on selenium automated chrome browser\n",
    "driver.get(\"https://images.google.com/\")\n",
    "\n",
    "item=driver.find_element(By.CLASS_NAME,\"gLFyf\")\n",
    "item.send_keys('Guitars')\n",
    "\n",
    "search=driver.find_element(By.XPATH,'//span[@class=\"z1asCe MZy1Rb\"]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b7bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    \n",
    "images=driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        break\n",
    "    print(\"Download {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])\n",
    "    file = open(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\7. Webscrapping-3\\Guitar\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e83d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "# opening the naukari page on selenium automated chrome browser\n",
    "driver.get(\"https://images.google.com/\")\n",
    "\n",
    "item=driver.find_element(By.CLASS_NAME,\"gLFyf\")\n",
    "item.send_keys('Cakes')\n",
    "\n",
    "search=driver.find_element(By.XPATH,'//span[@class=\"z1asCe MZy1Rb\"]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    \n",
    "images=driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        break\n",
    "    print(\"Download {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])\n",
    "    file = open(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\7. Webscrapping-3\\cake\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4ee25",
   "metadata": {},
   "source": [
    "# 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on\n",
    "www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be \n",
    "scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, \n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the \n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87015505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "# opening the flipkart page on selenium automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "# entering the Job position & Location as required\n",
    "products=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "products.send_keys('Oneplus Nord')\n",
    "\n",
    "cut=driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _2doB4z\"]')\n",
    "cut.click()\n",
    "\n",
    "search=driver.find_element(By.XPATH,'//button[@class=\"L0Z3Pu\"]')\n",
    "search.click()\n",
    "\n",
    "product_name = []\n",
    "colour = []\n",
    "ram_rom = []\n",
    "camera=[]\n",
    "display=[]\n",
    "batt_capacity=[]\n",
    "price1=[]\n",
    "product_url1=[]\n",
    "\n",
    "pn=driver.find_elements(By.XPATH,'//div[@class=\"_4rR01T\"]')\n",
    "for i in pn:\n",
    "    pn1=i.text\n",
    "    product_name.append(pn1)\n",
    "product_name\n",
    "\n",
    "product_name1 = [item.split('(')[0].strip() for item in product_name]\n",
    "product_name1\n",
    "\n",
    "colors_list = []\n",
    "\n",
    "for product in product_name:\n",
    "    # Split the string into brand, name, and details\n",
    "    split_data = product.split('(')\n",
    "    details = split_data[1].strip(')')\n",
    "\n",
    "    # Split the details into color and other attributes\n",
    "    color, _ = details.split(',', maxsplit=1)\n",
    "    \n",
    "    colors_list.append(color.strip())\n",
    "\n",
    "print(colors_list)\n",
    "\n",
    "ra=driver.find_elements(By.XPATH,'//li[@class=\"rgWa7D\"][1]')\n",
    "for i in ra:\n",
    "    ra1=i.text\n",
    "    ram_rom.append(ra1)\n",
    "ram_rom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_list = []\n",
    "rom_list = []\n",
    "\n",
    "for item in ram_rom:\n",
    "    ram, rom = item.split('|')\n",
    "    ram_list.append(ram.strip())\n",
    "    rom_list.append(rom.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp=driver.find_elements(By.XPATH,'//li[@class=\"rgWa7D\"][2]')\n",
    "for i in disp:\n",
    "    disp1=i.text\n",
    "    display.append(disp1)\n",
    "\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam=driver.find_elements(By.XPATH,'//li[@class=\"rgWa7D\"][3]')\n",
    "for i in cam:\n",
    "    cam1=i.text\n",
    "    camera.append(cam1)\n",
    "\n",
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c898b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity=driver.find_elements(By.XPATH,'//li[@class=\"rgWa7D\"][4]')\n",
    "for i in capacity:\n",
    "    batt=i.text\n",
    "    batt_capacity.append(batt)\n",
    "\n",
    "batt_capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3 _1_WHN1\"]')\n",
    "# for i in p1:\n",
    "#     p2=i.text\n",
    "#     price1.append(p2)\n",
    "# price1\n",
    "\n",
    "try:\n",
    "    p1=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3 _1_WHN1\"]')\n",
    "    for i in p1:\n",
    "        p2=i.text\n",
    "        price1.append(p2)\n",
    "except NoSuchElementException:\n",
    "    price1.append('-')\n",
    "    \n",
    "price1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_u=driver.find_elements(By.XPATH,'//a[@class = \"_1fQZEK\"]')\n",
    "for i in p_u:\n",
    "    p_u1=i.get_attribute('href')\n",
    "    product_url1.append(p_u1)\n",
    "product_url1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'Product Name':product_name1, 'Colour':colors_list, 'RAM':ram_list, 'ROM':rom_list, 'Camera':camera, 'Display':display, 'Battery Capacity':batt_capacity,'URL':product_url1})\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2f0981",
   "metadata": {},
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b9c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e968fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the google maps page on selenium automated chrome browser\n",
    "driver.get(\"https://www.google.com/maps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2fa901",
   "metadata": {},
   "outputs": [],
   "source": [
    "city=driver.find_element(By.XPATH,\"/html/body/div[3]/div[9]/div[3]/div[1]/div[1]/div[1]/div[2]/form/div[2]/div[3]/div/input[1]\")\n",
    "city.send_keys('Delhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2526c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,'//button[@class=\"mL3xi\"]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb114ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted:\", url_string)\n",
    "    coordinates = re.findall(r'@(.*)data', url_string)\n",
    "    print(\"Coordinates:\", coordinates)\n",
    "except Exception as e:\n",
    "    print(\"Error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate = coordinates[0].split(',')\n",
    "latitude = coordinate[0]\n",
    "longitude = coordinate[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c97de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a324c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a3d269",
   "metadata": {},
   "source": [
    "# 6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea56ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Deals not available on website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3888ec",
   "metadata": {},
   "source": [
    "# 7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "# opening the digit.in page on selenium automated chrome browser\n",
    "driver.get(\"https://www.digit.in/\")\n",
    "search_l=driver.find_element(By.XPATH,'/html/body/div[2]/div/ul/li[7]/div/div/div[2]/div/ul[1]/li[2]/a')\n",
    "search_l.click()\n",
    "search_bgl=driver.find_element(By.XPATH,'/html/body/div[7]/div/div/div[2]/div[1]/ul/li[9]/a')\n",
    "search_bgl.click()\n",
    "name = []\n",
    "os = []\n",
    "disp = []\n",
    "mem = []\n",
    "price = []\n",
    "Processor = []\n",
    "wt = []\n",
    "desc = []\n",
    "# Scrape all name tag\n",
    "name_tag  = driver.find_elements(By.XPATH, \"//*[@id='summtable']/tbody/tr/td[1]\")\n",
    "for n in name_tag:\n",
    "    name.append(n.text)\n",
    "    \n",
    "# Scrape all price tag\n",
    "price_tag  = driver.find_elements(By.XPATH, \"//*[@id='summtable']/tbody/tr/td[3]\")\n",
    "for p in price_tag:\n",
    "    price.append(p.text)\n",
    "    \n",
    "# Scrape all OS tag\n",
    "os_tag = driver.find_elements(By.XPATH, \"//div[@class='Spcs-details']/table/tbody/tr[4]/td[3]\")\n",
    "for o in os_tag:\n",
    "    os.append(o.text)\n",
    "    \n",
    "# Scrape all display tag\n",
    "disp_tag = driver.find_elements(By.XPATH, \"//div[@class='Spcs-details']/table/tbody/tr[3]/td[3]\")\n",
    "for d in disp_tag:\n",
    "    disp.append(d.text)\n",
    "    \n",
    "# Scrape all Processor tag\n",
    "Processor_tag = driver.find_elements(By.XPATH, \"//div[@class='Spcs-details']/table/tbody/tr[5]/td[3]\")\n",
    "for k in Processor_tag:\n",
    "    Processor.append(k.text)\n",
    "    \n",
    "# # Scrape all Graphic tag\n",
    "# GraphProc_tag = driver.find_elements(By.XPATH, \"//div[@class='value']\")\n",
    "# for g in GraphProc_tag:\n",
    "#     GraphProc.append(g.text)\n",
    "    \n",
    "# Scrape all memory tag\n",
    "mem_tag = driver.find_elements(By.XPATH, \"//div[@class='Spcs-details']/table/tbody/tr[6]/td[3]\")\n",
    "for m in mem_tag:\n",
    "    mem.append(m.text)\n",
    "        \n",
    "# Scrape all weight tag\n",
    "wt_tag = driver.find_elements(By.XPATH, \"//div[@class='Spcs-details']/table/tbody/tr[7]/td[3]\")\n",
    "for w in wt_tag:\n",
    "    wt.append(w.text)\n",
    "    \n",
    "# Scrape all description tag\n",
    "desc_tag = driver.find_elements(By.XPATH, \"//div[@class='Section-center']//p\")\n",
    "for des in desc_tag:\n",
    "    desc.append(des.text)\n",
    "Laptops = {'Laptop Name':name,'Operating System':os,'Display': disp,'Processor':Processor,'Memory':mem,'Dimension & Weight(kg)':wt,'Price(₹)':price}\n",
    "print(len(name),len(os),len(disp),len(Processor),len(mem),len(wt),len(price))\n",
    "bgl  = pd.DataFrame(Laptops)\n",
    "bgl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06668208",
   "metadata": {},
   "source": [
    "# 8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9efb375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.forbes.com/\")\n",
    "bill=driver.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div[1]/div/div[2]/ul/li[2]/div[2]/div[3]/ul/li[1]/a')\n",
    "bill.click()\n",
    "rank = []\n",
    "name = []\n",
    "networth = []\n",
    "age = []\n",
    "city = []\n",
    "source = []\n",
    "industry = []\n",
    "rank_tag = driver.find_elements(By.XPATH, \"//div[@class='Table_rank___YBhk Table_dataCell__2QCve']\")\n",
    "for r in rank_tag:\n",
    "    rank.append(r.text)\n",
    "name_tag = driver.find_elements(By.XPATH, \"//div[2][@class='TableRow_cell__db-hv Table_cell__houv9']\")\n",
    "for n in name_tag:\n",
    "    name.append(n.text)\n",
    "netwrth_tag = driver.find_elements(By.XPATH, \"//div[@class='Table_netWorth___L4R5 Table_dataCell__2QCve']\")\n",
    "for nt in netwrth_tag:\n",
    "    networth.append(nt.text)\n",
    "age_tag = driver.find_elements(By.XPATH, \"//div[4][@class='TableRow_cell__db-hv Table_cell__houv9']\")\n",
    "for a in age_tag:\n",
    "    age.append(a.text)\n",
    "cit_tag = driver.find_elements(By.XPATH, \"//div[5][@class='TableRow_cell__db-hv Table_cell__houv9']\")\n",
    "for c in cit_tag:\n",
    "    city.append(c.text)\n",
    "src_tag = driver.find_elements(By.XPATH, \"//div[6][@class='TableRow_cell__db-hv Table_cell__houv9']\")\n",
    "for s in src_tag:\n",
    "    source.append(s.text)\n",
    "ind_tag = driver.find_elements(By.XPATH, \"//div[7][@class='TableRow_cell__db-hv Table_cell__houv9']\")\n",
    "for i in ind_tag:\n",
    "    industry.append(i.text)\n",
    "print(len(rank),len(name),len(networth),len(age),len(city),len(source),len(industry))\n",
    "\n",
    "df4 = pd.DataFrame({'Rank':rank,'Name':name,'Net Worth':networth, 'Age':age,'Citizenship':city,'Source':source,'Industry':industry})\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878ea9c",
   "metadata": {},
   "source": [
    "# 9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.youtube.com/\")\n",
    "play=driver.find_element(By.XPATH,'/html/body/ytd-app/div[1]/ytd-page-manager/ytd-browse/ytd-two-column-browse-results-renderer/div[1]/ytd-rich-grid-renderer/div[6]/ytd-rich-grid-row[1]/div/ytd-rich-item-renderer[2]/div/ytd-rich-grid-media/div[1]/div[2]/div[1]/h3/a/yt-formatted-string')\n",
    "play.click()\n",
    "for _ in range(100):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "comments=driver.find_elements(By.XPATH,'//yt-formatted-string[@class=\"style-scope ytd-comment-renderer\"]')\n",
    "upvotes=driver.find_elements(By.XPATH,'//span[@class=\"style-scope ytd-comment-action-buttons-renderer\"]')\n",
    "times=driver.find_elements(By.XPATH,'//yt-formatted-string[@class=\"published-time-text style-scope ytd-comment-renderer\"]')\n",
    "\n",
    "comment = []\n",
    "upvote = []\n",
    "time =[]\n",
    "\n",
    "for i in comments:\n",
    "    comments1=i.text\n",
    "    comment.append(comments1)\n",
    "\n",
    "for i in upvotes:\n",
    "    upvotes1=i.text\n",
    "    upvote.append(upvotes1)\n",
    "    \n",
    "for i in times:\n",
    "    times1=i.text\n",
    "    time.append(times1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4daf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1271834",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_upvote = [item for item in upvote if item != '']\n",
    "\n",
    "print(c_upvote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe37144",
   "metadata": {},
   "outputs": [],
   "source": [
    "time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674d57d",
   "metadata": {},
   "source": [
    "# 10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.hostelworld.com/\")\n",
    "location=driver.find_element(By.CLASS_NAME,\"search-input\")\n",
    "location.send_keys('London')\n",
    "search=driver.find_element(By.XPATH,'//div[@class=\"label\"]')\n",
    "search.click()\n",
    "lets_go=driver.find_element(By.XPATH,'//button[@class=\"button primary large\"]')\n",
    "lets_go.click()\n",
    "h_name=driver.find_elements(By.XPATH,'//div[@class=\"property-name\"]')\n",
    "distance=driver.find_elements(By.XPATH,'//span[2][@class=\"distance-description\"]')\n",
    "rating=driver.find_elements(By.XPATH,'//div[@class=\"score-row\"]')\n",
    "review=driver.find_elements(By.XPATH,'//span[@class=\"left-margin\"]')\n",
    "o_review=driver.find_elements(By.XPATH,'//div[@class=\"keyword\"]')\n",
    "private_price=driver.find_elements(By.XPATH,'//strong[@class=\"current\"]')\n",
    "dorm_price=driver.find_elements(By.XPATH,'//strong[@class=\"current\"]')\n",
    "facility=driver.find_elements(By.XPATH,'//div[@class=\"icon-container badge-icon\"]')\n",
    "\n",
    "h_names = []\n",
    "distances = []\n",
    "ratings =[]\n",
    "reviews=[]\n",
    "o_reviews=[]\n",
    "privates_price=[]\n",
    "dorms_price=[]\n",
    "facilities=[]\n",
    "\n",
    "for i in h_name:\n",
    "    h_name1=i.text\n",
    "    h_names.append(h_name1)\n",
    "\n",
    "for i in distance:\n",
    "    distance1=i.text\n",
    "    distances.append(distance1)\n",
    "    \n",
    "for i in rating:\n",
    "    rating1=i.text\n",
    "    ratings.append(rating1)\n",
    "\n",
    "for i in review:\n",
    "    review1=i.text\n",
    "    reviews.append(review1)\n",
    "\n",
    "for i in o_review:\n",
    "    o_review1=i.text\n",
    "    o_reviews.append(o_review1)\n",
    "\n",
    "for i in private_price:\n",
    "    private_price1=i.text\n",
    "    privates_price.append(private_price1)\n",
    "\n",
    "for i in dorm_price:\n",
    "    dorm_price1=i.text\n",
    "    dorms_price.append(dorm_price1)\n",
    "\n",
    "for i in facility:\n",
    "    facility1=i.text\n",
    "    facilities.append(facility1)\n",
    "view=driver.find_element(By.XPATH,'//button[@class=\"button primary small full-width\"]')\n",
    "view.click()\n",
    "descr=driver.find_elements(By.XPATH,'//div[@class=\"content collapse-content\"]')\n",
    "\n",
    "description=[]\n",
    "\n",
    "for i in descr:\n",
    "    descr1=i.text\n",
    "    description.append(descr1)\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe66a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b921a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2430ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0cddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9d2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f14d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b8885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45519e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8e20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54578aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895db3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2d64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
