{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd40202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c8153",
   "metadata": {},
   "source": [
    "# 1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos.You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5876bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "# opening the page on selenium automated chrome browser\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")\n",
    "driver.maximize_window()\n",
    "\n",
    "rank=[]\n",
    "ranks=driver.find_elements(By.XPATH,'//table[2][@class=\"wikitable sortable jquery-tablesorter\"]/tbody/tr/td[1]')\n",
    "for i in ranks:\n",
    "    rank1=i.text\n",
    "    rank.append(rank1)\n",
    "ranking = [rank.rstrip('.') for rank in rank]\n",
    "\n",
    "name=[]\n",
    "names=driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr/td[2]')\n",
    "for i in names:\n",
    "    name1=i.text\n",
    "    name.append(name1)\n",
    "\n",
    "artist=[]\n",
    "artists=driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr/td[3]')\n",
    "for i in artists:\n",
    "    artist1=i.text\n",
    "    artist.append(artist1)\n",
    "    \n",
    "u_date=[]\n",
    "u_dates=driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr/td[5]')\n",
    "for i in u_dates:\n",
    "    u_date1=i.text\n",
    "    u_date.append(u_date1)\n",
    "    \n",
    "views=[]\n",
    "view=driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr/td[4]')\n",
    "for i in u_dates:\n",
    "    view1=i.text\n",
    "    views.append(view1)\n",
    "\n",
    "print(len(ranking),len(name),len(artist),len(u_date),len(views))\n",
    "\n",
    "df=pd.DataFrame({'Rank':ranking,'Name':name,'Artist':artist,'Upload Date':u_date,'Views':views})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12abbed",
   "metadata": {},
   "source": [
    "# 2. Scrape the details teamIndiaâ€™sinternationalfixtures from bcci.tv. Url = https://www.bcci.tv/.You need to find following details:\n",
    "A) Match title (I.e. 1stODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.bcci.tv/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "International = driver.find_element(By.XPATH, '//a[@class = \"nav-link \"]') # click button\n",
    "International.click()\n",
    "\n",
    "title=[]\n",
    "mt=driver.find_elements(By.XPATH,\"//span[@class='matchOrderText ng-binding ng-scope']\")\n",
    "for i in mt:\n",
    "    mt1=i.text\n",
    "    title.append(mt1)\n",
    "\n",
    "series=[]\n",
    "seriess=driver.find_elements(By.XPATH,\"//h5[@class='match-tournament-name ng-binding']\")\n",
    "for i in seriess:\n",
    "    series1=i.text\n",
    "    series.append(series1)\n",
    "    \n",
    "place=[]\n",
    "places=driver.find_elements(By.XPATH,\"//span[@class='ng-binding']\")\n",
    "for i in places:\n",
    "    place1=i.text\n",
    "    place.append(place1)\n",
    "    \n",
    "date=[]\n",
    "dates=driver.find_elements(By.XPATH,\"//div[@class='match-dates ng-binding']\")\n",
    "for i in dates:\n",
    "    date1=i.text\n",
    "    date.append(date1)\n",
    "\n",
    "time=[]\n",
    "times=driver.find_elements(By.XPATH,\"//div[@class='match-time no-margin ng-binding']\")\n",
    "for i in times:\n",
    "    time1=i.text\n",
    "    time.append(time1)\n",
    "    \n",
    "df1=pd.DataFrame({'Title':title,'Series':series,'Place':place,'Date':date,'Time':time})\n",
    "df1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3eca3",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of State-wise GDP ofIndia fromstatisticstime.com. Url = http://statisticstimes.com/\n",
    "    You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c6853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"http://statisticstimes.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# click on Economy\n",
    "\n",
    "economy = driver.find_element(By.XPATH, '//a[@href=\"economy/india-statistics.php\"]')\n",
    "try:\n",
    "    driver.get(economy.get_attribute('href'))\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(economy.get_attribute('href'))\n",
    "    \n",
    "# click on GDP of Indian states\n",
    "\n",
    "gdp = driver.find_element(By.XPATH, '//a[@href=\"india/indian-states-gdp.php\"]')\n",
    "try:\n",
    "    driver.get(gdp.get_attribute('href'))\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(gdp.get_attribute('href'))\n",
    "    \n",
    "rank=[]\n",
    "rnk=driver.find_elements(By.XPATH,\"//td[@class='data1']\")\n",
    "for i in rnk[:33]:\n",
    "    rnk1=i.text\n",
    "    rank.append(rnk1)\n",
    "\n",
    "state=[]\n",
    "ste=driver.find_elements(By.XPATH,\"//td[@class='name']\")\n",
    "for i in ste[:33]:\n",
    "    ste1=i.text\n",
    "    state.append(ste1)\n",
    "    \n",
    "gsdp1819=[]\n",
    "g1819=driver.find_elements(By.XPATH,\"//td[@class='data sorting_1']\")\n",
    "for i in g1819[:33]:\n",
    "    g1819s=i.text\n",
    "    gsdp1819.append(g1819s)\n",
    "\n",
    "gsdp1920=[]\n",
    "g1920=driver.find_elements(By.XPATH,\"//td[3][@class='data']\")\n",
    "for i in g1920[:33]:\n",
    "    g1920s=i.text\n",
    "    gsdp1920.append(g1920s)\n",
    "\n",
    "share=[]\n",
    "s1819=driver.find_elements(By.XPATH,\"//td[5][@class='data']\")\n",
    "for i in s1819[:33]:\n",
    "    s1819s=i.text\n",
    "    share.append(s1819s)\n",
    "    \n",
    "gdp=[]\n",
    "gdps=driver.find_elements(By.XPATH,\"//td[6][@class='data']\")\n",
    "for i in gdps[:33]:\n",
    "    gdp1=i.text\n",
    "    gdp.append(gdp1)\n",
    "    \n",
    "print(len(rank),len(state),len(gsdp1819),len(gsdp1920),len(share),len(gdp))\n",
    "\n",
    "df2= pd.DataFrame({'Rank':rank,'State':state,'GSDP(18-19)- at current prices':gsdp1819,'GSDP(19-20)- at current prices':gsdp1920,'Share(18-19)':share,'GDP($ billion)':gdp})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304e122",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of trending repositories on Github.com. Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://github.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# click on trending\n",
    "\n",
    "trending = driver.find_element(By.XPATH, '//*[@href=\"/trending\"]')\n",
    "try:\n",
    "    driver.get(trending.get_attribute('href'))\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(trending.get_attribute('href'))\n",
    "    \n",
    "title=[]\n",
    "rep=driver.find_elements(By.XPATH,\"/html/body/div[1]/div[4]/main/div[3]/div/div[2]/article/h2/a\")\n",
    "for i in rep:\n",
    "    name1=i.text\n",
    "    title.append(name1)\n",
    "    \n",
    "descr=[]\n",
    "des=driver.find_elements(By.XPATH,\"/html/body/div[1]/div[4]/main/div[3]/div/div[2]/article/p\")\n",
    "for i in des:\n",
    "    des1=i.text\n",
    "    descr.append(des1)\n",
    "    \n",
    "count=[]\n",
    "counts=driver.find_elements(By.XPATH,\"/html/body/div[1]/div[4]/main/div[3]/div/div[2]/article/div[2]/a[2]\")\n",
    "for i in counts:\n",
    "    count1=i.text\n",
    "    count.append(count1)\n",
    "    \n",
    "language=[]\n",
    "try:\n",
    "    languages=driver.find_elements(By.XPATH,\"//span[@class='d-inline-block ml-0 mr-3']\")\n",
    "    for i in languages:\n",
    "        language1=i.text\n",
    "        language.append(language1)\n",
    "except:\n",
    "    language.append('-')\n",
    "    \n",
    "# languages=[i.text for i in driver.find_elements(By.XPATH,\"//span[@class='d-inline-block ml-0 mr-3']\")]\n",
    "# for i in languages:\n",
    "#     try:\n",
    "#         language.append(i)\n",
    "#     except:\n",
    "#         language.append('-')\n",
    "\n",
    "print(len(title),len(descr),len(count),len(language))\n",
    "\n",
    "df3 = pd.DataFrame({'Repository Title':title,'Repository Description':descr,'Contributors Count':count,})\n",
    "#'Language Used':language\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e3cb2d",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e51760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.billboard.com/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "#Clicking on charts\n",
    "url=driver.find_elements(By.XPATH,\"//a[@class='c-link  lrv-a-unstyle-link lrv-u-font-size-18@desktop lrv-u-color-grey:hover u-color-brand-secondary:hover@desktop-max u-padding-a-1@desktop-max lrv-u-padding-tb-025 lrv-u-display-block']\")\n",
    "urls=url[0].get_attribute('href')\n",
    "driver.get(urls)\n",
    "\n",
    "# Show all charts by view charts\n",
    "button=driver.find_element(By.XPATH,'//a[@class=\"c-link  lrv-a-unstyle-link lrv-u-background-color-brand-secondary-dark lrv-u-color-grey-lightest lrv-u-color-grey-lightest:hover lrv-u-width-100p lrv-u-text-align-center lrv-a-hover-effect lrv-u-background-color-grey-dark:hover a-font-accent-fancy lrv-u-font-size-20 u-padding-tb-15 u-letter-spacing-0112 lrv-a-icon-after a-icon-magazine lrv-u-justify-content-center lrv-u-align-items-center lrv-a-unstyle-link u-width-100p@tablet u-width-235@mobile-max lrv-u-margin-t-050@mobile-max u-background-color-white@mobile-max u-color-black@mobile-max lrv-u-border-a-1@mobile-max lrv-u-border-color-brand-accent-red u-line-height-1\"]')\n",
    "button.click()\n",
    "\n",
    "Name=[]\n",
    "s1=driver.find_elements(By.XPATH,\"//h3[@class='c-title  a-no-trucate a-font-primary-bold-s u-letter-spacing-0021 u-font-size-23@tablet lrv-u-font-size-16 u-line-height-125 u-line-height-normal@mobile-max a-truncate-ellipsis u-max-width-245 u-max-width-230@tablet-only u-letter-spacing-0028@tablet']\")\n",
    "s2=driver.find_elements(By.XPATH,\"//h3[@class='c-title  a-no-trucate a-font-primary-bold-s u-letter-spacing-0021 lrv-u-font-size-18@tablet lrv-u-font-size-16 u-line-height-125 u-line-height-normal@mobile-max a-truncate-ellipsis u-max-width-330 u-max-width-230@tablet-only']\")\n",
    "names = s1 + s2\n",
    "for i in names:\n",
    "    name1=i.text\n",
    "    Name.append(name1)\n",
    "\n",
    "Artist=[]\n",
    "a1=driver.find_elements(By.XPATH,\"//span[@class='c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only u-font-size-20@tablet']\")\n",
    "a2=driver.find_elements(By.XPATH,\"//span[@class='c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only']\")\n",
    "artists= a1 + a2\n",
    "for i in artists:\n",
    "    artist1=i.text\n",
    "    Artist.append(artist1)\n",
    "\n",
    "lwrank=[]\n",
    "lwr1=driver.find_elements(By.XPATH,\"//li[4]/span[@class='c-label  a-font-primary-bold-l a-font-primary-m@mobile-max u-font-weight-normal@mobile-max lrv-u-padding-tb-050@mobile-max u-font-size-32@tablet']\")\n",
    "lwr2=driver.find_elements(By.XPATH,\"//li[4]/span[@class='c-label  a-font-primary-m lrv-u-padding-tb-050@mobile-max']\")\n",
    "lwranks= lwr1 + lwr2\n",
    "for i in lwranks:\n",
    "    lwrank1=i.text\n",
    "    lwrank.append(lwrank1)\n",
    "lw_rank = [x.strip() for x in lwrank if x.strip() != \"\"]\n",
    "\n",
    "peakrank=[]\n",
    "pr1=driver.find_elements(By.XPATH,\"//li[5]/span[@class='c-label  a-font-primary-bold-l a-font-primary-m@mobile-max u-font-weight-normal@mobile-max lrv-u-padding-tb-050@mobile-max u-font-size-32@tablet']\")\n",
    "pr2=driver.find_elements(By.XPATH,\"//li[5]/span[@class='c-label  a-font-primary-m lrv-u-padding-tb-050@mobile-max']\")\n",
    "peakranks= pr1 + pr2\n",
    "for i in peakranks:\n",
    "    peakrank1=i.text\n",
    "    peakrank.append(peakrank1)\n",
    "p_rank = [x.strip() for x in peakrank if x.strip() != \"\"]\n",
    "\n",
    "week=[]\n",
    "w1=driver.find_elements(By.XPATH,\"//li[6]/span[@class='c-label  a-font-primary-bold-l a-font-primary-m@mobile-max u-font-weight-normal@mobile-max lrv-u-padding-tb-050@mobile-max u-font-size-32@tablet']\")\n",
    "w2=driver.find_elements(By.XPATH,\"//li[6]/span[@class='c-label  a-font-primary-m lrv-u-padding-tb-050@mobile-max']\")\n",
    "weeks= w1 + w2\n",
    "for i in weeks:\n",
    "    week1=i.text\n",
    "    week.append(week1)\n",
    "    \n",
    "print(len(Name),len(Artist),len(lw_rank),len(p_rank),len(week))\n",
    "\n",
    "df4 = pd.DataFrame({'Song Name':Name,'Artist Name':Artist,'Last Week Rank':lw_rank,'Peak Position':p_rank,'Week on Charts':week})\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aab19d",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of Highest sellingnovels. Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "\n",
    "book=[]\n",
    "author=[]\n",
    "volume=[]\n",
    "publisher=[]\n",
    "genre=[]\n",
    "for j in range(1,101):\n",
    "    for i in driver.find_elements(By.XPATH, \"//tbody/tr[{}]/td[2]\".format(j)):\n",
    "        book.append(i.text)\n",
    "    for i in driver.find_elements(By.XPATH, \"//tbody/tr[{}]/td[3]\".format(j)):\n",
    "        author.append(i.text)\n",
    "    for i in driver.find_elements(By.XPATH, \"//tbody/tr[{}]/td[4]\".format(j)):\n",
    "        volume.append(i.text)\n",
    "    for i in driver.find_elements(By.XPATH, \"//tbody/tr[{}]/td[5]\".format(j)):\n",
    "        publisher.append(i.text)\n",
    "    for i in driver.find_elements(By.XPATH, \"//tbody/tr[{}]/td[6]\".format(j)):\n",
    "        genre.append(i.text)\n",
    "        \n",
    "print(len(book),len(author),len(volume),len(publisher),len(genre))\n",
    "\n",
    "df5 = pd.DataFrame({'Book Name':book,'Author Name':author,'Volumes Sold':volume,'Publisher':publisher,'Genre':genre})\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd840068",
   "metadata": {},
   "source": [
    "# 7. Scrape the details most watched tv series of all time from imdb.com. Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92909035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "\n",
    "Name=[]\n",
    "Year=[]\n",
    "Genre=[]\n",
    "Time=[]\n",
    "Rating=[]\n",
    "Votes=[]\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH, \"//div[@class='lister-item mode-detail']/div[2]/h3/a\"):\n",
    "        Name.append(i.text)\n",
    "except NoSuchAttributeException as e:\n",
    "    Name.append('--')\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH, \"//span[@class='lister-item-year text-muted unbold']\"):\n",
    "        Year.append(i.text)  \n",
    "except NoSuchAttributeException as e:\n",
    "     Year.append('--')\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH, \"//p[@class='text-muted text-small']/span[5]\"):\n",
    "        Genre.append(i.text)  \n",
    "except NoSuchAttributeException as e:\n",
    "    Genre.append('--')\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH, \"//p[@class='text-muted text-small']/span[3]\"):\n",
    "        Time.append(i.text)  \n",
    "except NoSuchAttributeException as e:\n",
    "    Time.append('--')  \n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH, \"//div[@class='ipl-rating-widget']/div/span[2]\"):\n",
    "        Rating.append(i.text)  \n",
    "except NoSuchAttributeException as e:\n",
    "    Rating.append('--')   \n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH, \"//span[@name='nv']\"):\n",
    "        Votes.append(i.text)  \n",
    "except NoSuchAttributeException as e:\n",
    "    Votes.append('--')\n",
    "    \n",
    "print(len(Name),len(Year),len(Genre),len(Time),len(Rating),len(Votes))\n",
    "\n",
    "df6 = pd.DataFrame({'Name':Name,'Year':Year,'Genre':Genre,'Time':Time,'Rating':Rating,'Votes':Votes})\n",
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34acaa98",
   "metadata": {},
   "source": [
    "# 8. Details of Datasetsfrom UCI machine learning repositories. Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "# opening the naukari page on selenium automated chrome browser\n",
    "driver.get(\"https://archive.ics.uci.edu/\")\n",
    "\n",
    "search=driver.find_element(By.XPATH,'//span[2][@class=\"whitetext\"]/a/font/b')\n",
    "search.click()\n",
    "\n",
    "dataset=[]\n",
    "datasets = driver.find_elements(By.XPATH, '//p[@class=\"normal\"]/b/a')\n",
    "for i in datasets:\n",
    "    dataset1=i.text\n",
    "    dataset.append(dataset1)\n",
    "\n",
    "dtype = []\n",
    "dtypes = driver.find_elements(By.XPATH, '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]/p')\n",
    "for i in dtypes[1:624]:\n",
    "    dtype1=i.text\n",
    "    dtype.append(dtype1)\n",
    "    \n",
    "task =[]\n",
    "tasks = driver.find_elements(By.XPATH, '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[3]/p')\n",
    "for i in tasks[1:624]:\n",
    "    task1=i.text\n",
    "    task.append(task1)\n",
    "    \n",
    "atype =[]\n",
    "atypes = driver.find_elements(By.XPATH, '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]/p')\n",
    "for i in atypes[1:624]:\n",
    "    atype1=i.text\n",
    "    atype.append(atype1)\n",
    "    \n",
    "instances = []\n",
    "instance = driver.find_elements(By.XPATH, '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[5]/p')\n",
    "for i in instance[1:624]:\n",
    "    instance1=i.text\n",
    "    instances.append(instance1)\n",
    "    \n",
    "attributes = []\n",
    "attribute = driver.find_elements(By.XPATH, '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[6]/p')\n",
    "for i in attribute[1:624]:\n",
    "    attribute1=i.text\n",
    "    attributes.append(attribute1)\n",
    "    \n",
    "year = []\n",
    "years = driver.find_elements(By.XPATH, '/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[7]/p')\n",
    "for i in years[1:624]:\n",
    "    year1=i.text\n",
    "    year.append(year1)\n",
    "    \n",
    "print(len(dataset),len(dtype),len(task),len(atype),len(instances),len(attributes),len(year))\n",
    "\n",
    "df7 = pd.DataFrame({'Dataset':dataset,'Data type':dtype,'Task':task, 'Attribute type':atype,'No of Instances':instances,'No of Attribute':attributes,'YEAR':year})\n",
    "df7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d979b",
   "metadata": {},
   "source": [
    "# 9. Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants\n",
    "You have to find the following details: \n",
    "A) Name\n",
    "B) Designation\n",
    "C)Company \n",
    "D)Skills they hire for \n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and \n",
    "click on search. All this should be done through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver\n",
    "driver=webdriver.Chrome(r\"E:\\Surendra Data\\DATA Scientist Course Detail\\chromedriver.exe\")\n",
    "\n",
    "# opening the naukari page on selenium automated chrome browser\n",
    "driver.get(\"https://www.naukri.com/hr-recruiters-consultants\")\n",
    "\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME,\"sugInp\")\n",
    "designation.send_keys('Data Science')\n",
    "\n",
    "search=driver.find_element(By.XPATH,'//button[@class=\"fl qsbSrch blueBtn\"]')\n",
    "search.click()\n",
    "\n",
    "name=[]\n",
    "desig=[]\n",
    "comp=[]\n",
    "loc=[]\n",
    "recruiter=[i.text.split('\\n') for i in driver.find_elements(By.XPATH,\"//p[@class='highlightable']\")]\n",
    "for i in recruiter:\n",
    "    try:\n",
    "        name.append(i[0])\n",
    "    except:\n",
    "        name.append('-')\n",
    "    try:\n",
    "        desig.append(i[1])\n",
    "    except:\n",
    "        desig.append('-')\n",
    "    try:\n",
    "        comp.append(i[2])\n",
    "    except:\n",
    "        comp.append('-')\n",
    "    try:\n",
    "        loc.append(i[3])\n",
    "    except:\n",
    "        loc.append('-')\n",
    "        \n",
    "# names = driver.find_elements(By.XPATH, '//span[@class=\"fl ellipsis\"]')\n",
    "# for i in names:\n",
    "#     name1=i.text\n",
    "#     name.append(name1)\n",
    "    \n",
    "# desi = driver.find_elements(By.XPATH, '//span[@class=\"ellipsis clr\"]')\n",
    "# for i in desi:\n",
    "#     desi1=i.text\n",
    "#     designation.append(desi1)\n",
    "    \n",
    "# comp = driver.find_elements(By.XPATH, '//a[2][@class=\"ellipsis\"]')\n",
    "# for i in comp:\n",
    "#     comp1=i.text\n",
    "#     company.append(comp1)\n",
    "skills=[]    \n",
    "skill = driver.find_elements(By.XPATH, '//div[@class=\"hireSec highlightable\"]')\n",
    "for i in skill:\n",
    "    skill1=i.text\n",
    "    skills.append(skill1)\n",
    "    \n",
    "print(len(name),len(desig),len(comp),len(loc),len(skills))\n",
    "\n",
    "df8 = pd.DataFrame({'Name':name,'Designation':desig,'Company':comp, 'Skills for Hire':skills,'Location':loc})\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f9daf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
